{"cells":[{"cell_type":"markdown","id":"be2c0c13","metadata":{"id":"be2c0c13"},"source":["# LAB 2: Evaluating Generative Models\n","\n","In this lab, we are going to practice how to evaluate generative models.\n","The evaluation method is not limited to diffusion models, in fact, deep generative models such as GAN and Flow use this method widely.\n","\n","<span style=\"color:red\">It is necessary to have a GPU to complete LAB2.</span><br>\n","<span style=\"color:red\">Its efficacy will not affect your completion of this lab. Colab free plan also works.</span>"]},{"cell_type":"markdown","id":"0e2d6e27","metadata":{"id":"0e2d6e27"},"source":["## Install Packages\n","\n","Install `torch`, `torchvision` and `pytorch_gan_metrics`.\n","\n","- Colab\n","\n","    Use system default `torch` and `torchvision` to avoid hardware incompatibility.\n","    ```\n","    pip install pytorch_gan_metrics\n","    ```\n","\n","- Custom environment (include `conda` users)\n","\n","    Install all packages from pypi.\n","    ```\n","    pip install torch torchvision pytorch_gan_metrics\n","    ```"]},{"cell_type":"code","execution_count":1,"id":"HV-VXylCZk0A","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4777,"status":"ok","timestamp":1668484979793,"user":{"displayName":"黃丰楷","userId":"07551684872768698364"},"user_tz":-480},"id":"HV-VXylCZk0A","outputId":"fd0ef9ca-0003-452c-ceed-3f5ea6976e64"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_gan_metrics\n","  Downloading pytorch_gan_metrics-0.5.2-py3-none-any.whl (18 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pytorch_gan_metrics) (1.7.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_gan_metrics) (4.64.1)\n","Requirement already satisfied: torch>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_gan_metrics) (1.12.1+cu113)\n","Requirement already satisfied: torchvision>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_gan_metrics) (0.13.1+cu113)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch_gan_metrics) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.2->pytorch_gan_metrics) (4.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.9.2->pytorch_gan_metrics) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.9.2->pytorch_gan_metrics) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.9.2->pytorch_gan_metrics) (7.1.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch_gan_metrics) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.9.2->pytorch_gan_metrics) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.9.2->pytorch_gan_metrics) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.9.2->pytorch_gan_metrics) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.9.2->pytorch_gan_metrics) (2.10)\n","Installing collected packages: pytorch-gan-metrics\n","Successfully installed pytorch-gan-metrics-0.5.2\n"]}],"source":["!pip install pytorch_gan_metrics"]},{"cell_type":"code","execution_count":2,"id":"O5Tqfuc-coFj","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21068,"status":"ok","timestamp":1668485006686,"user":{"displayName":"黃丰楷","userId":"07551684872768698364"},"user_tz":-480},"id":"O5Tqfuc-coFj","outputId":"d858f4e9-3c79-473d-cd3a-aeb87528223b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","id":"33d6f513","metadata":{"id":"33d6f513"},"source":["## Checkpoint 1 - Generating Fake Images\n","- Use `torch` to generate some noise as images.\n","- Use `torchvision.utils.save_image` to save `torch.Tensor` as an `png` image file."]},{"cell_type":"code","execution_count":3,"id":"53f9e586","metadata":{"executionInfo":{"elapsed":2731,"status":"ok","timestamp":1668485012532,"user":{"displayName":"黃丰楷","userId":"07551684872768698364"},"user_tz":-480},"id":"53f9e586"},"outputs":[],"source":["import os\n","import torch\n","from torchvision.utils import save_image\n","\n","num_images = 1000\n","os.makedirs('./images', exist_ok=True)\n","for i in range(num_images):\n","    image = torch.randn((3, 28, 28))           # random normal\n","    image = torch.clamp(image, min=-1, max=1)  # clamp [-1, 1]\n","    image = (image + 1) / 2                    # shift [0 , 1]\n","    path = os.path.join(f'./images/{i:05d}.png')\n","    save_image(image,path)"]},{"cell_type":"markdown","id":"33fea212","metadata":{"id":"33fea212"},"source":["## (Optional) Check the Number of Files in `./images` (Unix Only)\n","- The number of files should be 1000\n","- Note that the number of generated images in spec of HW3 is 10000."]},{"cell_type":"code","execution_count":4,"id":"b9979c9f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":405,"status":"ok","timestamp":1668485025171,"user":{"displayName":"黃丰楷","userId":"07551684872768698364"},"user_tz":-480},"id":"b9979c9f","outputId":"ce9b1bc8-93be-4001-de57-8668ba143771"},"outputs":[{"name":"stdout","output_type":"stream","text":["1000\n"]}],"source":["!ls ./images | wc -l"]},{"cell_type":"markdown","id":"c42ad6b7","metadata":{"id":"c42ad6b7"},"source":["## Checkpoint 2 - Evaluate Generated Images in Console\n","\n","1. Download `mnist.npz` from E3 [Homework 3 - Source Code](https://e3.nycu.edu.tw/mod/assign/view.php?id=329368).\n","2. Refer to the example in page 5 of `HW3.pdf`.\n","3. Calculate the FID between generated images in `./images` and our dataset (`mnist.npz`).\n","\n","The calculated FID *must* be around $400$."]},{"cell_type":"code","execution_count":6,"id":"a3d974a4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28578,"status":"ok","timestamp":1668485060008,"user":{"displayName":"黃丰楷","userId":"07551684872768698364"},"user_tz":-480},"id":"a3d974a4","outputId":"3b9bebba-f39f-4e76-ed7f-3acc6df2fb47"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.245573461038032 0.03614834346961117 23.742484980302038                        \n"]}],"source":["!python -m pytorch_gan_metrics.calc_metrics \\\n","--path ./final_generated_images/FID43.4299949854059_OriginUnet2_Unet1_2_2_v2_batch64_lr0.0005_T500/images/ \\\n","--stat ./mnist.npz"]},{"cell_type":"markdown","id":"cd166708","metadata":{"id":"cd166708"},"source":["## Checkpoint 3 - Evaluate Generated Images at Runtime\n","- Use `torchvision.io.read_image` to load all images into memory one by one.\n","- Use `pytorch_gan_metrics.get_fid` to calculate FID between generated images in `./images` and our dataset (`mnist.npz`).\n","\n","    The following is the document about how to use `get_fid` for reference.\n","    ```python\n","    from pytorch_gan_metrics import get_fid\n","\n","    images = ...                                    # [N, 3, H, W] normalized to [0, 1]\n","    FID = get_fid(images, 'path/to/statistics.npz') # Frechet Inception Distance\n","    ```\n","\n","The output FID should be the same as Checkpoint 2 (round to two decimal places)."]},{"cell_type":"code","execution_count":5,"id":"536956d9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19255,"status":"ok","timestamp":1668485196633,"user":{"displayName":"黃丰楷","userId":"07551684872768698364"},"user_tz":-480},"id":"536956d9","outputId":"335cf057-6404-447c-bf23-aa48a8b3b872"},"outputs":[{"ename":"RuntimeError","evalue":"\"upsample_bilinear2d_out_frame\" not implemented for 'Byte'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn [5], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     images\u001b[39m.\u001b[39mappend(image)\n\u001b[1;32m     10\u001b[0m images \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(images, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m FID \u001b[39m=\u001b[39m get_fid(images,\u001b[39m'\u001b[39;49m\u001b[39m./mnist.npz\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mFID\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n","File \u001b[0;32m~/CCBDA/HW3/CCBDA_HW3env/lib/python3.8/site-packages/pytorch_gan_metrics/utils.py:149\u001b[0m, in \u001b[0;36mget_fid\u001b[0;34m(images, fid_stats_path, use_torch, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_fid\u001b[39m(\n\u001b[1;32m    131\u001b[0m     images: Union[torch\u001b[39m.\u001b[39mFloatTensor, DataLoader],\n\u001b[1;32m    132\u001b[0m     fid_stats_path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    133\u001b[0m     use_torch: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    134\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    135\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m    136\u001b[0m     \u001b[39m\"\"\"Calculate Frechet Inception Distance.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39m        FID\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     acts, \u001b[39m=\u001b[39m get_inception_feature(\n\u001b[1;32m    150\u001b[0m         images, dims\u001b[39m=\u001b[39;49m[\u001b[39m2048\u001b[39;49m], use_torch\u001b[39m=\u001b[39;49muse_torch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    152\u001b[0m     \u001b[39m# Frechet Inception Distance\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     f \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(fid_stats_path)\n","File \u001b[0;32m~/CCBDA/HW3/CCBDA_HW3env/lib/python3.8/site-packages/pytorch_gan_metrics/core.py:80\u001b[0m, in \u001b[0;36mget_inception_feature\u001b[0;34m(images, dims, batch_size, use_torch, verbose, device)\u001b[0m\n\u001b[1;32m     78\u001b[0m batch_images \u001b[39m=\u001b[39m batch_images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     79\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 80\u001b[0m     outputs \u001b[39m=\u001b[39m model(batch_images)\n\u001b[1;32m     81\u001b[0m     \u001b[39mfor\u001b[39;00m feature, output, dim \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(features, outputs, dims):\n\u001b[1;32m     82\u001b[0m         \u001b[39mif\u001b[39;00m use_torch:\n","File \u001b[0;32m~/CCBDA/HW3/CCBDA_HW3env/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/CCBDA/HW3/CCBDA_HW3env/lib/python3.8/site-packages/pytorch_gan_metrics/inception.py:160\u001b[0m, in \u001b[0;36mInceptionV3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    157\u001b[0m outputs \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_blocks))]\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize_input:\n\u001b[0;32m--> 160\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49minterpolate(x,\n\u001b[1;32m    161\u001b[0m                       size\u001b[39m=\u001b[39;49m(\u001b[39m299\u001b[39;49m, \u001b[39m299\u001b[39;49m),\n\u001b[1;32m    162\u001b[0m                       mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbilinear\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    163\u001b[0m                       align_corners\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_input:\n\u001b[1;32m    166\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m x \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# Scale from range (0, 1) to range (-1, 1)\u001b[39;00m\n","File \u001b[0;32m~/CCBDA/HW3/CCBDA_HW3env/lib/python3.8/site-packages/torch/nn/functional.py:3950\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3948\u001b[0m     \u001b[39mif\u001b[39;00m antialias:\n\u001b[1;32m   3949\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39m_upsample_bilinear2d_aa(\u001b[39minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[0;32m-> 3950\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mupsample_bilinear2d(\u001b[39minput\u001b[39;49m, output_size, align_corners, scale_factors)\n\u001b[1;32m   3951\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrilinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   3952\u001b[0m     \u001b[39massert\u001b[39;00m align_corners \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: \"upsample_bilinear2d_out_frame\" not implemented for 'Byte'"]}],"source":["from pytorch_gan_metrics import get_fid\n","from torchvision.io import read_image\n","import torch\n","\n","images = []\n","for i in range(10000):\n","    path = os.path.join(f'./final_generated_images/FID43.4299949854059_OriginUnet2_Unet1_2_2_v2_batch64_lr0.0005_T500/images/{i+1:05d}.png')\n","    image = read_image(path) \n","    images.append(image)\n","images = torch.stack(images, dim=0)\n","FID = get_fid(images,'./mnist.npz')\n","print(f'{FID:.5f}')"]},{"cell_type":"markdown","id":"f83b21df","metadata":{"id":"f83b21df"},"source":["---\n","\n","## Use `torchvision` to Save Grid Images"]},{"cell_type":"code","execution_count":null,"id":"202e10ba","metadata":{"id":"202e10ba","outputId":"4daaa954-586e-4641-b8bf-0a0c91d3f90b"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPIAAADyCAIAAAD4Cg/zAAAFYklEQVR4nO3c0ecQZh/G4btXoygaRdEoFu2gWBRFURSNRVEUi0axKIqiUQdFUSw2isbGYlEUGxvFxsZGY6MONjaKoiiKomj0nvz+hOf1e92u6w+4n5PPwffoSQD4/zflf/3A1KlTh2/++++/SaZNmzZ8+eXLl0lmzJgxfPn58+dJZs2aNXz56dOnSWbPnj18+fHjx0nmzp07fPnhw4dJ5s+fP3z5/v37Sf4zfBcmnawpJGsKyZpCsqaQrCkkawrJmkKyppCsKSRrCsmaQrKmkKwpJGsKyZpCsqaQrCkkawrJmkKyppCsKSRrCsmaQrKmkKwpJGsAYFJM/Ji6YMGC4dN3795N8vbbbw9f/ueff5IsXrx4+PJff/2VZMmSJcOXb9++neTdd98dvvzHH38kWb58+fDl3377LcnKlSuHL//6669JVq9ePXz5559/jtuaSrKmkKwpJGsKyZpCsqaQrCkkawrJmkKyppCsKSRrCsmaQrKmkKwpJGsKyZpCsqaQrCkkawrJmkKyppCsKSRrCsmaQrKmkKwBgEkx8WPq2rVrh0//+OOPSdavXz98+caNG0k2btw4fPn7779P8v777w9f/vbbb5Ns3rx5+PK1a9eSbN26dfjylStXkmzfvn348qVLl5J88MEHw5cvXrwYtzWVZE0hWVNI1hSSNYVkTSFZU0jWFJI1hWRNIVlTSNYUkjWFZE0hWVNI1hSSNYVkTSFZU0jWFJI1hWRNIVlTSNYUkjWFZE0hWQMAk2Lix9Rdu3YNn/7yyy+T7N69e/jy559/nuSjjz4avnz+/Pkk+/btG7782WefJTlw4MDw5bNnzyY5dOjQ8OUzZ84kOXLkyPDlU6dOJTl69Ojw5RMnTsRtTSVZU0jWFJI1hWRNIVlTSNYUkjWFZE0hWVNI1hSSNYVkTSFZU0jWFJI1hWRNIVlTSNYUkjWFZE0hWVNI1hSSNYVkTSFZU0jWAMCkmPgx9fjx48Onjx07luTkyZPDlz/++OMkp0+fHr58+PDhJJ988snw5YMHDyb59NNPhy/v378/yblz54Yv7927N8mFCxeGL+/ZsyfJF198MXz5ww8/jNuaSrKmkKwpJGsKyZpCsqaQrCkkawrJmkKyppCsKSRrCsmaQrKmkKwpJGsKyZpCsqaQrCkkawrJmkKyppCsKSRrCsmaQrKmkKwBgEkx8WPqV199NXx6586dSb7++uvhyzt27Ehy+fLl4cvbtm1LcvXq1eHLW7ZsSfLNN98MX960aVOS7777bvjye++9l+T69evDlzds2JDkhx9+GL68bt26uK2pJGsKyZpCsqaQrCkkawrJmkKyppCsKSRrCsmaQrKmkKwpJGsKyZpCsqaQrCkkawrJmkKyppCsKSRrCsmaQrKmkKwpJGsKyRoAmBQTP6b+9NNPw6fXrFmT5Jdffhm+vGrVqiQ3b94cvrxixYokv//++/DlZcuWJbl169bw5aVLlyb5888/hy+/8847Sf7+++/hy4sWLUpy586d4csLFy6M25pKsqaQrCkkawrJmkKyppCsKSRrCsmaQrKmkKwpJGsKyZpCsqaQrCkkawrJmkKyppCsKSRrCsmaQrKmkKwpJGsKyZpCsqaQrAGASTHxY+q9e/eGT7/11ltJHjx4MHx53rx5SR49ejR8ec6cOUmePHkyfPnNN99M8uzZs+HLM2fOTPLixYvhy9OnT0/y6tWr4ctvvPFGktevXw9fnjJlStzWVJI1hWRNIVlTSNYUkjWFZE0hWVNI1hSSNYVkTSFZU0jWFJI1hWRNIVlTSNYUkjWFZE0hWVNI1hSSNYVkTSFZU0jWFJI1hWQNAEyK/wJAGL65UypHcwAAAABJRU5ErkJggg==","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"}],"source":["from IPython.display import Image, display\n","\n","colors = torch.linspace(0, 1, 64)\n","images = colors.view(64, 1, 1, 1).expand(64, 3, 28, 28)\n","save_image(images, 'example.png')\n","display(Image(filename='example.png'))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"CCBDA_HW3env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"6ae799f72a4385797fde21c86378e06ea9254954dab69d368062e58ffff4bf49"}}},"nbformat":4,"nbformat_minor":5}
